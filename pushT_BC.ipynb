{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkothari3/lerobot_policy_bc/blob/main/pushT_BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQUk3Y0WwYZ4"
      },
      "source": [
        "# Train BC\n",
        "* Dataset trained on: https://huggingface.co/datasets/lerobot/pusht\n",
        "* Using Colab's T4\n",
        "* Custom Policy - Behavior Cloning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlKjL1X5t_zM"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgLu7QT5tUik",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/rkothari3/lerobot_policy_bc.git /content/lerobot_policy_bc\n",
        "!cd /content/lerobot_policy_bc && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"from lerobot_policy_bc import BC, BCConfig; print('✅ BC Policy installed successfully!')\"\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tvFk-iUfN4Pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n",
        "!conda install ffmpeg=7.1.1 -c conda-forge\n",
        "!cd lerobot && pip install -e ."
      ],
      "metadata": {
        "id": "j-amMARpT48p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd lerobot && pip install -e \".[pusht]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "fdmJE1RGOyBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/lerobot/src/lerobot/policies/factory.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import importlib\n",
        "import logging\n",
        "from typing import Any, TypedDict\n",
        "\n",
        "import torch\n",
        "from typing_extensions import Unpack\n",
        "\n",
        "from lerobot.configs.policies import PreTrainedConfig\n",
        "from lerobot.configs.types import FeatureType\n",
        "from lerobot.datasets.lerobot_dataset import LeRobotDatasetMetadata\n",
        "from lerobot.datasets.utils import dataset_to_policy_features\n",
        "from lerobot.envs.configs import EnvConfig\n",
        "from lerobot.envs.utils import env_to_policy_features\n",
        "from lerobot.policies.act.configuration_act import ACTConfig\n",
        "from lerobot.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
        "from lerobot.policies.groot.configuration_groot import GrootConfig\n",
        "from lerobot.policies.pi0.configuration_pi0 import PI0Config\n",
        "from lerobot.policies.pi05.configuration_pi05 import PI05Config\n",
        "from lerobot.policies.pretrained import PreTrainedPolicy\n",
        "from lerobot.policies.sac.configuration_sac import SACConfig\n",
        "from lerobot.policies.sac.reward_model.configuration_classifier import RewardClassifierConfig\n",
        "from lerobot.policies.sarm.configuration_sarm import SARMConfig\n",
        "from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig\n",
        "from lerobot.policies.tdmpc.configuration_tdmpc import TDMPCConfig\n",
        "from lerobot.policies.utils import validate_visual_features_consistency\n",
        "from lerobot.policies.vqbet.configuration_vqbet import VQBeTConfig\n",
        "from lerobot.policies.wall_x.configuration_wall_x import WallXConfig\n",
        "from lerobot.policies.xvla.configuration_xvla import XVLAConfig\n",
        "from lerobot.processor import PolicyAction, PolicyProcessorPipeline\n",
        "from lerobot.processor.converters import (\n",
        "    batch_to_transition,\n",
        "    policy_action_to_transition,\n",
        "    transition_to_batch,\n",
        "    transition_to_policy_action,\n",
        ")\n",
        "from lerobot.utils.constants import POLICY_POSTPROCESSOR_DEFAULT_NAME, POLICY_PREPROCESSOR_DEFAULT_NAME\n",
        "\n",
        "\n",
        "def get_policy_class(name: str) -> type[PreTrainedPolicy]:\n",
        "    \"\"\"\n",
        "    Retrieves a policy class by its registered name.\n",
        "\n",
        "    This function uses dynamic imports to avoid loading all policy classes into memory\n",
        "    at once, improving startup time and reducing dependencies.\n",
        "\n",
        "    Args:\n",
        "        name: The name of the policy. Supported names are \"tdmpc\", \"diffusion\", \"act\",\n",
        "              \"vqbet\", \"pi0\", \"pi05\", \"sac\", \"reward_classifier\", \"smolvla\", \"wall_x\".\n",
        "\n",
        "    Returns:\n",
        "        The policy class corresponding to the given name.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If the policy name is not recognized.\n",
        "    \"\"\"\n",
        "    if name == \"tdmpc\":\n",
        "        from lerobot.policies.tdmpc.modeling_tdmpc import TDMPCPolicy\n",
        "\n",
        "        return TDMPCPolicy\n",
        "    elif name == \"diffusion\":\n",
        "        from lerobot.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
        "\n",
        "        return DiffusionPolicy\n",
        "    elif name == \"act\":\n",
        "        from lerobot.policies.act.modeling_act import ACTPolicy\n",
        "\n",
        "        return ACTPolicy\n",
        "    elif name == \"vqbet\":\n",
        "        from lerobot.policies.vqbet.modeling_vqbet import VQBeTPolicy\n",
        "\n",
        "        return VQBeTPolicy\n",
        "    elif name == \"pi0\":\n",
        "        from lerobot.policies.pi0.modeling_pi0 import PI0Policy\n",
        "\n",
        "        return PI0Policy\n",
        "    elif name == \"pi05\":\n",
        "        from lerobot.policies.pi05.modeling_pi05 import PI05Policy\n",
        "\n",
        "        return PI05Policy\n",
        "    elif name == \"sac\":\n",
        "        from lerobot.policies.sac.modeling_sac import SACPolicy\n",
        "\n",
        "        return SACPolicy\n",
        "    elif name == \"reward_classifier\":\n",
        "        from lerobot.policies.sac.reward_model.modeling_classifier import Classifier\n",
        "\n",
        "        return Classifier\n",
        "    elif name == \"smolvla\":\n",
        "        from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy\n",
        "\n",
        "        return SmolVLAPolicy\n",
        "    elif name == \"sarm\":\n",
        "        from lerobot.policies.sarm.modeling_sarm import SARMRewardModel\n",
        "\n",
        "        return SARMRewardModel\n",
        "    elif name == \"groot\":\n",
        "        from lerobot.policies.groot.modeling_groot import GrootPolicy\n",
        "\n",
        "        return GrootPolicy\n",
        "    elif name == \"xvla\":\n",
        "        from lerobot.policies.xvla.modeling_xvla import XVLAPolicy\n",
        "\n",
        "        return XVLAPolicy\n",
        "    elif name == \"wall_x\":\n",
        "        from lerobot.policies.wall_x.modeling_wall_x import WallXPolicy\n",
        "        return WallXPolicy\n",
        "    elif name == \"bc\":\n",
        "        from lerobot_policy_bc import BC\n",
        "        return BC\n",
        "    else:\n",
        "        try:\n",
        "            return _get_policy_cls_from_policy_name(name=name)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Policy type '{name}' is not available.\") from e\n",
        "\n",
        "\n",
        "def make_policy_config(policy_type: str, **kwargs) -> PreTrainedConfig:\n",
        "    \"\"\"\n",
        "    Instantiates a policy configuration object based on the policy type.\n",
        "\n",
        "    This factory function simplifies the creation of policy configuration objects by\n",
        "    mapping a string identifier to the corresponding config class.\n",
        "\n",
        "    Args:\n",
        "        policy_type: The type of the policy. Supported types include \"tdmpc\",\n",
        "                     \"diffusion\", \"act\", \"vqbet\", \"pi0\", \"pi05\", \"sac\", \"smolvla\",\n",
        "                     \"reward_classifier\", \"wall_x\".\n",
        "        **kwargs: Keyword arguments to be passed to the configuration class constructor.\n",
        "\n",
        "    Returns:\n",
        "        An instance of a `PreTrainedConfig` subclass.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the `policy_type` is not recognized.\n",
        "    \"\"\"\n",
        "    if policy_type == \"tdmpc\":\n",
        "        return TDMPCConfig(**kwargs)\n",
        "    elif policy_type == \"diffusion\":\n",
        "        return DiffusionConfig(**kwargs)\n",
        "    elif policy_type == \"act\":\n",
        "        return ACTConfig(**kwargs)\n",
        "    elif policy_type == \"vqbet\":\n",
        "        return VQBeTConfig(**kwargs)\n",
        "    elif policy_type == \"pi0\":\n",
        "        return PI0Config(**kwargs)\n",
        "    elif policy_type == \"pi05\":\n",
        "        return PI05Config(**kwargs)\n",
        "    elif policy_type == \"sac\":\n",
        "        return SACConfig(**kwargs)\n",
        "    elif policy_type == \"smolvla\":\n",
        "        return SmolVLAConfig(**kwargs)\n",
        "    elif policy_type == \"reward_classifier\":\n",
        "        return RewardClassifierConfig(**kwargs)\n",
        "    elif policy_type == \"groot\":\n",
        "        return GrootConfig(**kwargs)\n",
        "    elif policy_type == \"xvla\":\n",
        "        return XVLAConfig(**kwargs)\n",
        "    elif policy_type == \"wall_x\":\n",
        "        return WallXConfig(**kwargs)\n",
        "    else:\n",
        "        try:\n",
        "            config_cls = PreTrainedConfig.get_choice_class(policy_type)\n",
        "            return config_cls(**kwargs)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Policy type '{policy_type}' is not available.\") from e\n",
        "\n",
        "\n",
        "class ProcessorConfigKwargs(TypedDict, total=False):\n",
        "    \"\"\"\n",
        "    A TypedDict defining the keyword arguments for processor configuration.\n",
        "\n",
        "    This provides type hints for the optional arguments passed to `make_pre_post_processors`,\n",
        "    improving code clarity and enabling static analysis.\n",
        "\n",
        "    Attributes:\n",
        "        preprocessor_config_filename: The filename for the preprocessor configuration.\n",
        "        postprocessor_config_filename: The filename for the postprocessor configuration.\n",
        "        preprocessor_overrides: A dictionary of overrides for the preprocessor configuration.\n",
        "        postprocessor_overrides: A dictionary of overrides for the postprocessor configuration.\n",
        "        dataset_stats: Dataset statistics for normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    preprocessor_config_filename: str | None\n",
        "    postprocessor_config_filename: str | None\n",
        "    preprocessor_overrides: dict[str, Any] | None\n",
        "    postprocessor_overrides: dict[str, Any] | None\n",
        "    dataset_stats: dict[str, dict[str, torch.Tensor]] | None\n",
        "\n",
        "\n",
        "def make_pre_post_processors(\n",
        "    policy_cfg: PreTrainedConfig,\n",
        "    pretrained_path: str | None = None,\n",
        "    **kwargs: Unpack[ProcessorConfigKwargs],\n",
        ") -> tuple[\n",
        "    PolicyProcessorPipeline[dict[str, Any], dict[str, Any]],\n",
        "    PolicyProcessorPipeline[PolicyAction, PolicyAction],\n",
        "]:\n",
        "    \"\"\"\n",
        "    Create or load pre- and post-processor pipelines for a given policy.\n",
        "\n",
        "    This function acts as a factory. It can either load existing processor pipelines\n",
        "    from a pretrained path or create new ones from scratch based on the policy\n",
        "    configuration. Each policy type has a dedicated factory function for its\n",
        "    processors (e.g., `make_tdmpc_pre_post_processors`).\n",
        "\n",
        "    Args:\n",
        "        policy_cfg: The configuration of the policy for which to create processors.\n",
        "        pretrained_path: An optional path to load pretrained processor pipelines from.\n",
        "            If provided, pipelines are loaded from this path.\n",
        "        **kwargs: Keyword arguments for processor configuration, as defined in\n",
        "            `ProcessorConfigKwargs`.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the input (pre-processor) and output (post-processor) pipelines.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If a processor factory is not implemented for the given\n",
        "            policy configuration type.\n",
        "    \"\"\"\n",
        "    if pretrained_path:\n",
        "        # TODO(Steven): Temporary patch, implement correctly the processors for Gr00t\n",
        "        if isinstance(policy_cfg, GrootConfig):\n",
        "            # GROOT handles normalization in groot_pack_inputs_v3 step\n",
        "            # Need to override both stats AND normalize_min_max since saved config might be empty\n",
        "            preprocessor_overrides = {}\n",
        "            postprocessor_overrides = {}\n",
        "            preprocessor_overrides[\"groot_pack_inputs_v3\"] = {\n",
        "                \"stats\": kwargs.get(\"dataset_stats\"),\n",
        "                \"normalize_min_max\": True,\n",
        "            }\n",
        "\n",
        "            # Also ensure postprocessing slices to env action dim and unnormalizes with dataset stats\n",
        "            env_action_dim = policy_cfg.output_features[\"action\"].shape[0]\n",
        "            postprocessor_overrides[\"groot_action_unpack_unnormalize_v1\"] = {\n",
        "                \"stats\": kwargs.get(\"dataset_stats\"),\n",
        "                \"normalize_min_max\": True,\n",
        "                \"env_action_dim\": env_action_dim,\n",
        "            }\n",
        "            kwargs[\"preprocessor_overrides\"] = preprocessor_overrides\n",
        "            kwargs[\"postprocessor_overrides\"] = postprocessor_overrides\n",
        "\n",
        "        return (\n",
        "            PolicyProcessorPipeline.from_pretrained(\n",
        "                pretrained_model_name_or_path=pretrained_path,\n",
        "                config_filename=kwargs.get(\n",
        "                    \"preprocessor_config_filename\", f\"{POLICY_PREPROCESSOR_DEFAULT_NAME}.json\"\n",
        "                ),\n",
        "                overrides=kwargs.get(\"preprocessor_overrides\", {}),\n",
        "                to_transition=batch_to_transition,\n",
        "                to_output=transition_to_batch,\n",
        "            ),\n",
        "            PolicyProcessorPipeline.from_pretrained(\n",
        "                pretrained_model_name_or_path=pretrained_path,\n",
        "                config_filename=kwargs.get(\n",
        "                    \"postprocessor_config_filename\", f\"{POLICY_POSTPROCESSOR_DEFAULT_NAME}.json\"\n",
        "                ),\n",
        "                overrides=kwargs.get(\"postprocessor_overrides\", {}),\n",
        "                to_transition=policy_action_to_transition,\n",
        "                to_output=transition_to_policy_action,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    # Create a new processor based on policy type\n",
        "    if isinstance(policy_cfg, TDMPCConfig):\n",
        "        from lerobot.policies.tdmpc.processor_tdmpc import make_tdmpc_pre_post_processors\n",
        "\n",
        "        processors = make_tdmpc_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, DiffusionConfig):\n",
        "        from lerobot.policies.diffusion.processor_diffusion import make_diffusion_pre_post_processors\n",
        "\n",
        "        processors = make_diffusion_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, ACTConfig):\n",
        "        from lerobot.policies.act.processor_act import make_act_pre_post_processors\n",
        "\n",
        "        processors = make_act_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, VQBeTConfig):\n",
        "        from lerobot.policies.vqbet.processor_vqbet import make_vqbet_pre_post_processors\n",
        "\n",
        "        processors = make_vqbet_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, PI0Config):\n",
        "        from lerobot.policies.pi0.processor_pi0 import make_pi0_pre_post_processors\n",
        "\n",
        "        processors = make_pi0_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, PI05Config):\n",
        "        from lerobot.policies.pi05.processor_pi05 import make_pi05_pre_post_processors\n",
        "\n",
        "        processors = make_pi05_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, SACConfig):\n",
        "        from lerobot.policies.sac.processor_sac import make_sac_pre_post_processors\n",
        "\n",
        "        processors = make_sac_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, RewardClassifierConfig):\n",
        "        from lerobot.policies.sac.reward_model.processor_classifier import make_classifier_processor\n",
        "\n",
        "        processors = make_classifier_processor(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, SmolVLAConfig):\n",
        "        from lerobot.policies.smolvla.processor_smolvla import make_smolvla_pre_post_processors\n",
        "\n",
        "        processors = make_smolvla_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, SARMConfig):\n",
        "        from lerobot.policies.sarm.processor_sarm import make_sarm_pre_post_processors\n",
        "\n",
        "        processors = make_sarm_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "            dataset_meta=kwargs.get(\"dataset_meta\"),\n",
        "        )\n",
        "    elif isinstance(policy_cfg, GrootConfig):\n",
        "        from lerobot.policies.groot.processor_groot import make_groot_pre_post_processors\n",
        "\n",
        "        processors = make_groot_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, XVLAConfig):\n",
        "        from lerobot.policies.xvla.processor_xvla import (\n",
        "            make_xvla_pre_post_processors,\n",
        "        )\n",
        "\n",
        "        processors = make_xvla_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    elif isinstance(policy_cfg, WallXConfig):\n",
        "        from lerobot.policies.wall_x.processor_wall_x import make_wall_x_pre_post_processors\n",
        "\n",
        "        processors = make_wall_x_pre_post_processors(\n",
        "            config=policy_cfg,\n",
        "            dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        try:\n",
        "            processors = _make_processors_from_policy_config(\n",
        "                config=policy_cfg,\n",
        "                dataset_stats=kwargs.get(\"dataset_stats\"),\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Processor for policy type '{policy_cfg.type}' is not implemented.\") from e\n",
        "\n",
        "    return processors\n",
        "\n",
        "\n",
        "def make_policy(\n",
        "    cfg: PreTrainedConfig,\n",
        "    ds_meta: LeRobotDatasetMetadata | None = None,\n",
        "    env_cfg: EnvConfig | None = None,\n",
        "    rename_map: dict[str, str] | None = None,\n",
        ") -> PreTrainedPolicy:\n",
        "    \"\"\"\n",
        "    Instantiate a policy model.\n",
        "\n",
        "    This factory function handles the logic of creating a policy, which requires\n",
        "    determining the input and output feature shapes. These shapes can be derived\n",
        "    either from a `LeRobotDatasetMetadata` object or an `EnvConfig` object. The function\n",
        "    can either initialize a new policy from scratch or load a pretrained one.\n",
        "\n",
        "    Args:\n",
        "        cfg: The configuration for the policy to be created. If `cfg.pretrained_path` is\n",
        "             set, the policy will be loaded with weights from that path.\n",
        "        ds_meta: Dataset metadata used to infer feature shapes and types. Also provides\n",
        "                 statistics for normalization layers.\n",
        "        env_cfg: Environment configuration used to infer feature shapes and types.\n",
        "                 One of `ds_meta` or `env_cfg` must be provided.\n",
        "        rename_map: Optional mapping of dataset or environment feature keys to match\n",
        "                 expected policy feature names (e.g., `\"left\"` → `\"camera1\"`).\n",
        "\n",
        "    Returns:\n",
        "        An instantiated and device-placed policy model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If both or neither of `ds_meta` and `env_cfg` are provided.\n",
        "        NotImplementedError: If attempting to use an unsupported policy-backend\n",
        "                             combination (e.g., VQBeT with 'mps').\n",
        "    \"\"\"\n",
        "    if bool(ds_meta) == bool(env_cfg):\n",
        "        raise ValueError(\"Either one of a dataset metadata or a sim env must be provided.\")\n",
        "\n",
        "    # NOTE: Currently, if you try to run vqbet with mps backend, you'll get this error.\n",
        "    # TODO(aliberts, rcadene): Implement a check_backend_compatibility in policies?\n",
        "    # NotImplementedError: The operator 'aten::unique_dim' is not currently implemented for the MPS device. If\n",
        "    # you want this op to be added in priority during the prototype phase of this feature, please comment on\n",
        "    # https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment\n",
        "    # variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be\n",
        "    # slower than running natively on MPS.\n",
        "    if cfg.type == \"vqbet\" and cfg.device == \"mps\":\n",
        "        raise NotImplementedError(\n",
        "            \"Current implementation of VQBeT does not support `mps` backend. \"\n",
        "            \"Please use `cpu` or `cuda` backend.\"\n",
        "        )\n",
        "\n",
        "    policy_cls = get_policy_class(cfg.type)\n",
        "\n",
        "    kwargs = {}\n",
        "    if ds_meta is not None:\n",
        "        features = dataset_to_policy_features(ds_meta.features)\n",
        "    else:\n",
        "        if not cfg.pretrained_path:\n",
        "            logging.warning(\n",
        "                \"You are instantiating a policy from scratch and its features are parsed from an environment \"\n",
        "                \"rather than a dataset. Normalization modules inside the policy will have infinite values \"\n",
        "                \"by default without stats from a dataset.\"\n",
        "            )\n",
        "        if env_cfg is None:\n",
        "            raise ValueError(\"env_cfg cannot be None when ds_meta is not provided\")\n",
        "        features = env_to_policy_features(env_cfg)\n",
        "\n",
        "    cfg.output_features = {key: ft for key, ft in features.items() if ft.type is FeatureType.ACTION}\n",
        "    if not cfg.input_features:\n",
        "        cfg.input_features = {key: ft for key, ft in features.items() if key not in cfg.output_features}\n",
        "    kwargs[\"config\"] = cfg\n",
        "\n",
        "    # Pass dataset_stats to the policy if available (needed for some policies like SARM)\n",
        "    if ds_meta is not None and hasattr(ds_meta, \"stats\"):\n",
        "        kwargs[\"dataset_stats\"] = ds_meta.stats\n",
        "\n",
        "    if ds_meta is not None:\n",
        "        kwargs[\"dataset_meta\"] = ds_meta\n",
        "\n",
        "    if cfg.pretrained_path:\n",
        "        # Load a pretrained policy and override the config if needed (for example, if there are inference-time\n",
        "        # hyperparameters that we want to vary).\n",
        "        kwargs[\"pretrained_name_or_path\"] = cfg.pretrained_path\n",
        "        policy = policy_cls.from_pretrained(**kwargs)\n",
        "    else:\n",
        "        # Make a fresh policy.\n",
        "        policy = policy_cls(**kwargs)\n",
        "\n",
        "    policy.to(cfg.device)\n",
        "    assert isinstance(policy, torch.nn.Module)\n",
        "\n",
        "    # policy = torch.compile(policy, mode=\"reduce-overhead\")\n",
        "\n",
        "    if not rename_map:\n",
        "        validate_visual_features_consistency(cfg, features)\n",
        "        # TODO: (jadechoghari) - add a check_state(cfg, features) and check_action(cfg, features)\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def _get_policy_cls_from_policy_name(name: str) -> type[PreTrainedConfig]:\n",
        "    \"\"\"Get policy class from its registered name using dynamic imports.\n",
        "\n",
        "    This is used as a helper function to import policies from 3rd party lerobot plugins.\n",
        "\n",
        "    Args:\n",
        "        name: The name of the policy.\n",
        "    Returns:\n",
        "        The policy class corresponding to the given name.\n",
        "    \"\"\"\n",
        "    if name not in PreTrainedConfig.get_known_choices():\n",
        "        raise ValueError(\n",
        "            f\"Unknown policy name '{name}'. Available policies: {PreTrainedConfig.get_known_choices()}\"\n",
        "        )\n",
        "\n",
        "    config_cls = PreTrainedConfig.get_choice_class(name)\n",
        "    config_cls_name = config_cls.__name__\n",
        "\n",
        "    model_name = config_cls_name.removesuffix(\"Config\")  # e.g., DiffusionConfig -> Diffusion\n",
        "    if model_name == config_cls_name:\n",
        "        raise ValueError(\n",
        "            f\"The config class name '{config_cls_name}' does not follow the expected naming convention.\"\n",
        "            f\"Make sure it ends with 'Config'!\"\n",
        "        )\n",
        "    cls_name = model_name + \"Policy\"  # e.g., DiffusionConfig -> DiffusionPolicy\n",
        "    module_path = config_cls.__module__.replace(\n",
        "        \"configuration_\", \"modeling_\"\n",
        "    )  # e.g., configuration_diffusion -> modeling_diffusion\n",
        "\n",
        "    module = importlib.import_module(module_path)\n",
        "    policy_cls = getattr(module, cls_name)\n",
        "    return policy_cls\n",
        "\n",
        "\n",
        "def _make_processors_from_policy_config(\n",
        "    config: PreTrainedConfig,\n",
        "    dataset_stats: dict[str, dict[str, torch.Tensor]] | None = None,\n",
        ") -> tuple[Any, Any]:\n",
        "    \"\"\"Create pre- and post-processors from a policy configuration using dynamic imports.\n",
        "\n",
        "    This is used as a helper function to import processor factories from 3rd party lerobot plugins.\n",
        "\n",
        "    Args:\n",
        "        config: The policy configuration object.\n",
        "        dataset_stats: Dataset statistics for normalization.\n",
        "    Returns:\n",
        "        A tuple containing the input (pre-processor) and output (post-processor) pipelines.\n",
        "    \"\"\"\n",
        "\n",
        "    policy_type = config.type\n",
        "    function_name = f\"make_{policy_type}_pre_post_processors\"\n",
        "    module_path = config.__class__.__module__.replace(\n",
        "        \"configuration_\", \"processor_\"\n",
        "    )  # e.g., configuration_diffusion -> processor_diffusion\n",
        "    logging.debug(\n",
        "        f\"Instantiating pre/post processors using function '{function_name}' from module '{module_path}'\"\n",
        "    )\n",
        "    module = importlib.import_module(module_path)\n",
        "    function = getattr(module, function_name)\n",
        "    return function(config, dataset_stats=dataset_stats)\n"
      ],
      "metadata": {
        "id": "Z4HDUBM7VFlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python lerobot/src/lerobot/scripts/lerobot_train.py \\\n",
        "  --dataset.repo_id=lerobot/pusht \\\n",
        "  --policy.type=bc \\\n",
        "  --batch_size=64 \\\n",
        "  --steps=50000 \\\n",
        "  --output_dir=outputs/train/pushT_BC \\\n",
        "  --job_name=pushT_BC \\\n",
        "  --policy.device=cuda \\\n",
        "  --policy.use_amp=true \\\n",
        "  --eval_freq=5000 \\\n",
        "  --save_freq=10000 \\\n",
        "  --policy.push_to_hub=false \\\n",
        "  --num_workers=2 \\\n",
        "  --wandb.enable=false"
      ],
      "metadata": {
        "id": "6TPtMTnCUhOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test with 5 episodes\n",
        "!cd lerobot && python src/lerobot/scripts/lerobot_eval.py \\\n",
        "  --policy.path=rkothari3/pushT_BC \\\n",
        "  --output_dir=outputs/eval/quick_test \\\n",
        "  --env.type=pusht \\\n",
        "  --eval.n_episodes=5 \\\n",
        "  --eval.batch_size=5"
      ],
      "metadata": {
        "id": "K4zEaYZbV-9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yu5khQGIHi6"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFMLGuVkH7UN"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli upload rkothari3/pushT_BC \\\n",
        "  /content/lerobot/outputs/train/pushT_BC/checkpoints/last/pretrained_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}